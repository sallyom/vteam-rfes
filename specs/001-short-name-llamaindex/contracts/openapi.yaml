openapi: 3.0.3
info:
  title: Llamaindex + RamaLama RAG Demo API
  description: |
    REST API for the RAG demonstration system integrating Llamaindex orchestration,
    RamaLama local LLM serving, and docs2db database infrastructure.

    This API enables:
    - Natural language queries over indexed document collections
    - Source-cited responses with relevance scoring
    - System health and statistics monitoring
    - Configurable retrieval parameters

    **Architecture**: FastAPI + Llamaindex + PostgreSQL/pgvector + RamaLama
  version: 0.1.0
  contact:
    name: API Support
    email: support@example.com

servers:
  - url: http://localhost:8000
    description: Local development server
  - url: http://rag-demo:8000
    description: Container deployment

tags:
  - name: query
    description: RAG query processing endpoints
  - name: system
    description: Health and monitoring endpoints

paths:
  /query:
    post:
      tags:
        - query
      summary: Submit RAG query
      description: |
        Process a natural language query against the indexed document collection.

        **Processing Flow**:
        1. Query text embedded using same model as documents
        2. Vector similarity search retrieves relevant chunks
        3. Chunks filtered by similarity threshold
        4. Context assembled with token budget management
        5. LLM generates response with source citations

        **Performance**: Target <5s end-to-end latency
      operationId: submitQuery
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/QueryRequest'
            examples:
              basic_query:
                summary: Basic query with defaults
                value:
                  query: "What are the system requirements?"
              configured_query:
                summary: Query with custom parameters
                value:
                  query: "How do I install the application?"
                  top_k: 7
                  similarity_threshold: 0.75
                  enable_hybrid: true
      responses:
        '200':
          description: Query processed successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/QueryResponse'
              examples:
                successful_response:
                  summary: Successful query with sources
                  value:
                    query_id: "a3bb189e-8bf9-4b5a-b6b5-8a21a86f7e4f"
                    answer: |
                      The system requires Python 3.12 or higher, 4GB RAM, and 20GB storage. [Source: installation_guide.pdf, chunk 3]
                      PostgreSQL with pgvector extension is needed for vector storage. [Source: technical_overview.md, chunk 12]
                    sources:
                      - chunk_id: 145
                        text: "System requirements: Python 3.12+, 4GB RAM minimum, 20GB..."
                        similarity_score: 0.87
                        source_file: "installation_guide.pdf"
                        chunk_index: 3
                      - chunk_id: 289
                        text: "The vector database requires PostgreSQL 14+ with pgvector..."
                        similarity_score: 0.82
                        source_file: "technical_overview.md"
                        chunk_index: 12
                    confidence: "high"
                    processing_time_ms: 3247
                    metadata:
                      chunks_retrieved: 5
                      chunks_used: 2
                      retrieval_time_ms: 187
                      generation_time_ms: 2950
                      llm_model: "tinyllama-1.1b"
                not_found_response:
                  summary: No relevant information found
                  value:
                    query_id: "f1c3e5a7-2d4b-4f8a-9c1e-6b3a8d5f7e2a"
                    answer: "I don't have relevant information about that in the indexed documents. Top similarity score: 0.54 (threshold: 0.70)"
                    sources: []
                    confidence: "none"
                    processing_time_ms: 421
                    metadata:
                      chunks_retrieved: 5
                      chunks_used: 0
                      retrieval_time_ms: 201
                      generation_time_ms: 0
                      llm_model: "tinyllama-1.1b"
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                empty_query:
                  summary: Empty query text
                  value:
                    error: "Query text cannot be empty"
                    details:
                      field: "query"
                      constraint: "min_length: 3"
                invalid_top_k:
                  summary: Invalid top_k parameter
                  value:
                    error: "top_k must be between 1 and 20"
                    details:
                      field: "top_k"
                      value: 50
        '500':
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                database_error:
                  summary: Database connection failure
                  value:
                    error: "Database connection failed"
                    suggestion: "Check that PostgreSQL is running on port 5432"
                llm_error:
                  summary: LLM generation failure
                  value:
                    error: "LLM generation failed"
                    suggestion: "Check that RamaLama is running and model is loaded"

  /query/stream:
    post:
      tags:
        - query
      summary: Submit RAG query with streaming response
      description: |
        Process query with Server-Sent Events streaming for progressive response disclosure.

        **Benefits**:
        - Immediate feedback (perceived performance improvement)
        - Progressive answer disclosure
        - Source citations available immediately

        **Event Types**:
        - `token`: Individual token from LLM generation
        - `sources`: Retrieved source citations (sent first)
        - `done`: Completion event with metadata
      operationId: submitQueryStream
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/QueryRequest'
      responses:
        '200':
          description: SSE stream of response tokens
          content:
            text/event-stream:
              schema:
                type: string
              example: |
                event: sources
                data: {"sources": [{"chunk_id": 145, "similarity_score": 0.87, "source_file": "guide.pdf"}]}

                event: token
                data: {"delta": "The", "accumulated": "The"}

                event: token
                data: {"delta": " system", "accumulated": "The system"}

                event: done
                data: {"processing_time_ms": 3421, "confidence": "high"}
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'

  /health:
    get:
      tags:
        - system
      summary: System health check
      description: |
        Reports readiness status of all system components.

        **Components Checked**:
        - PostgreSQL database connection
        - Embedding model validation
        - RamaLama LLM server availability

        Returns `200 OK` only if all components are operational.
      operationId: getHealth
      responses:
        '200':
          description: System is healthy
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthResponse'
              examples:
                healthy:
                  summary: All components operational
                  value:
                    status: "healthy"
                    timestamp: "2025-10-30T14:23:45Z"
                    components:
                      database:
                        status: "connected"
                        latency_ms: 12
                      embedding_model:
                        status: "validated"
                        model: "ibm-granite/granite-embedding-30m-english"
                        dimensions: 384
                      llm_server:
                        status: "loaded"
                        model: "tinyllama-1.1b"
                        endpoint: "http://localhost:8080/v1"
        '503':
          description: System is unhealthy
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthResponse'
              examples:
                unhealthy:
                  summary: Component failure
                  value:
                    status: "unhealthy"
                    timestamp: "2025-10-30T14:25:12Z"
                    components:
                      database:
                        status: "connected"
                        latency_ms: 15
                      embedding_model:
                        status: "validated"
                        model: "granite-30m-english"
                        dimensions: 384
                      llm_server:
                        status: "unavailable"
                        error: "Connection refused to http://localhost:8080/v1"
                    suggestion: "Check that RamaLama container is running"

  /stats:
    get:
      tags:
        - system
      summary: Database and system statistics
      description: |
        Provides observability into the indexed document collection and system usage.

        **Metrics Provided**:
        - Document and chunk counts
        - Embedding model information
        - Query history statistics
        - Performance metrics
      operationId: getStats
      responses:
        '200':
          description: Statistics retrieved successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/StatsResponse'
              examples:
                demo_stats:
                  summary: Demo system statistics
                  value:
                    database:
                      documents: 127
                      chunks: 3847
                      embeddings: 3847
                      embedding_model:
                        name: "ibm-granite/granite-embedding-30m-english"
                        dimensions: 384
                        provider: "sentence-transformers"
                    queries:
                      total: 42
                      last_24h: 18
                      avg_latency_ms: 3125
                      p95_latency_ms: 4782
                    retrieval:
                      avg_similarity_score: 0.76
                      avg_chunks_retrieved: 5.2
                      avg_chunks_used: 3.1
                    llm:
                      model: "tinyllama-1.1b"
                      avg_tokens_generated: 127
                      avg_generation_time_ms: 2847

components:
  schemas:
    QueryRequest:
      type: object
      required:
        - query
      properties:
        query:
          type: string
          minLength: 3
          maxLength: 1000
          description: Natural language question about indexed documents
          example: "What are the system requirements for installation?"
        top_k:
          type: integer
          minimum: 1
          maximum: 20
          default: 5
          description: Number of chunks to retrieve from vector database
          example: 5
        similarity_threshold:
          type: number
          format: float
          minimum: 0.0
          maximum: 1.0
          default: 0.7
          description: Minimum similarity score for chunk inclusion (0.0-1.0)
          example: 0.7
        enable_hybrid:
          type: boolean
          default: false
          description: Enable hybrid search (dense vector + sparse BM25)
          example: false

    QueryResponse:
      type: object
      required:
        - query_id
        - answer
        - sources
        - confidence
        - processing_time_ms
      properties:
        query_id:
          type: string
          format: uuid
          description: Unique identifier for this query
          example: "a3bb189e-8bf9-4b5a-b6b5-8a21a86f7e4f"
        answer:
          type: string
          description: Generated response with inline source citations
          example: "The system requires Python 3.12 or higher. [Source: guide.pdf, chunk 3]"
        sources:
          type: array
          items:
            $ref: '#/components/schemas/SourceChunk'
          description: Retrieved source chunks with relevance scores
        confidence:
          type: string
          enum: [high, medium, low, none]
          description: |
            Confidence level based on top similarity score:
            - high: >= 0.8
            - medium: 0.7-0.8
            - low: 0.6-0.7
            - none: No relevant chunks found
          example: "high"
        processing_time_ms:
          type: integer
          description: End-to-end query processing time in milliseconds
          example: 3247
        metadata:
          type: object
          description: Additional processing metadata
          properties:
            chunks_retrieved:
              type: integer
              description: Total chunks retrieved from vector search
            chunks_used:
              type: integer
              description: Chunks included in LLM context
            retrieval_time_ms:
              type: integer
              description: Vector search latency
            generation_time_ms:
              type: integer
              description: LLM generation latency
            llm_model:
              type: string
              description: LLM model used for generation

    SourceChunk:
      type: object
      required:
        - chunk_id
        - text
        - similarity_score
        - source_file
      properties:
        chunk_id:
          type: integer
          description: Database chunk identifier
          example: 145
        text:
          type: string
          maxLength: 500
          description: Chunk text content (truncated for response)
          example: "System requirements: Python 3.12+, 4GB RAM minimum..."
        similarity_score:
          type: number
          format: float
          minimum: 0.0
          maximum: 1.0
          description: Cosine similarity score with query
          example: 0.87
        source_file:
          type: string
          description: Original source document filename
          example: "installation_guide.pdf"
        chunk_index:
          type: integer
          description: Position within source document
          example: 3

    HealthResponse:
      type: object
      required:
        - status
        - timestamp
        - components
      properties:
        status:
          type: string
          enum: [healthy, unhealthy]
          description: Overall system health status
          example: "healthy"
        timestamp:
          type: string
          format: date-time
          description: Health check timestamp
          example: "2025-10-30T14:23:45Z"
        components:
          type: object
          properties:
            database:
              type: object
              properties:
                status:
                  type: string
                  enum: [connected, disconnected]
                latency_ms:
                  type: integer
            embedding_model:
              type: object
              properties:
                status:
                  type: string
                  enum: [validated, mismatch]
                model:
                  type: string
                dimensions:
                  type: integer
            llm_server:
              type: object
              properties:
                status:
                  type: string
                  enum: [loaded, unavailable]
                model:
                  type: string
                endpoint:
                  type: string
        suggestion:
          type: string
          description: Remediation suggestion if unhealthy
          example: "Check that RamaLama container is running"

    StatsResponse:
      type: object
      required:
        - database
      properties:
        database:
          type: object
          properties:
            documents:
              type: integer
              description: Total indexed documents
            chunks:
              type: integer
              description: Total chunks in database
            embeddings:
              type: integer
              description: Total embeddings stored
            embedding_model:
              type: object
              properties:
                name:
                  type: string
                dimensions:
                  type: integer
                provider:
                  type: string
        queries:
          type: object
          properties:
            total:
              type: integer
              description: Total queries processed
            last_24h:
              type: integer
              description: Queries in last 24 hours
            avg_latency_ms:
              type: integer
              description: Average query latency
            p95_latency_ms:
              type: integer
              description: 95th percentile latency
        retrieval:
          type: object
          properties:
            avg_similarity_score:
              type: number
              format: float
              description: Average similarity of retrieved chunks
            avg_chunks_retrieved:
              type: number
              format: float
            avg_chunks_used:
              type: number
              format: float
        llm:
          type: object
          properties:
            model:
              type: string
            avg_tokens_generated:
              type: integer
            avg_generation_time_ms:
              type: integer

    ErrorResponse:
      type: object
      required:
        - error
      properties:
        error:
          type: string
          description: User-friendly error message
          example: "Database connection failed"
        details:
          type: object
          description: Additional error context (debug mode only)
        suggestion:
          type: string
          description: Actionable remediation suggestion
          example: "Check that PostgreSQL is running on port 5432"
